The objective of this coruse will basically boil down to "Efficiency".
How do you train the best model given a fixed set of resources(data+hardware(compute, memory, n/w bandwidth)) ?

To achieve the abvoe, we need to focus on some design decisions that can be categorized in to these, you dont need to know all these things in advance and neither do i at this stage of learning.

Basics
  - Tokenization
  - Architecture
  - Loss Function
  - Optimizer
  - Learning rate
Systems
  - Kernels
  - Parallelism
  - Quantization
  - Activation Checkpointing
  - CPU offloading
  - Inference
Scaling laws
  - Scaling Sequnce
  - Modeling complexity
  - Loss metric
  - Parametric form
Data
  - Evaluation
  - Curation
  - Transformation
  - Filtering
  - Deduplication
  - Mixing
Alignment
  - Supervised fine-tuning
  - Reinforcement learnng
  - Preference data
  - Synthetic data
  - Verifiers

Goal: Get a basic version of the full pipeline orking
Components: tokenization, model architecture and training

