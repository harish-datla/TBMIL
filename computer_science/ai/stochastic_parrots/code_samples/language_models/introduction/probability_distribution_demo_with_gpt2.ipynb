{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8916864c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "# transformers: Provides pre-trained models and tokenizers like GPT-2.\n",
    "#   GPT2LMHeadModel is the GPT-2 model with a language modeling head.\n",
    "#   GPT2Tokenizer is used to convert text into tokens that the model can understand.\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "# torch: PyTorch library, used for tensor computations and neural network modeling.\n",
    "import torch\n",
    "# math: Standard Python library for mathematical functions, used here for math.exp().\n",
    "import math\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "# model_name specifies which version of GPT-2 to use (e.g., \"gpt2\", \"gpt2-medium\").\n",
    "model_name = \"gpt2\"\n",
    "# Load the tokenizer associated with the specified GPT-2 model.\n",
    "# The tokenizer converts input strings into a format (token IDs) understandable by the model.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "# Load the pre-trained GPT-2 model (GPT2LMHeadModel for language modeling tasks).\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "# Set the model to evaluation mode.\n",
    "# This disables layers like dropout and batch normalization that behave differently during training and inference.\n",
    "# It's crucial for getting deterministic and correct results when using the model for inference.\n",
    "model.eval()\n",
    "\n",
    "# Define a function to compute the log probability of a sentence\n",
    "# This version is updated to prepend BOS token for more accurate probability calculation.\n",
    "def compute_log_probability(sentence_text, model, tokenizer):\n",
    "    # For GPT-2, the End-Of-Sentence (EOS) token is often used as the Begin-Of-Sentence (BOS) token\n",
    "    # when scoring sentences or calculating perplexity, as GPT-2 wasn't explicitly trained with a BOS token.\n",
    "    bos_id = tokenizer.eos_token_id \n",
    "    \n",
    "    # Tokenize the input sentence text to get a list of token IDs.\n",
    "    token_ids_list = tokenizer(sentence_text, return_tensors=\"pt\")[\"input_ids\"][0].tolist()\n",
    "    \n",
    "    # Prepend the BOS token ID to the sequence of token IDs.\n",
    "    # This forms the input that the model will process.\n",
    "    model_input_ids = torch.tensor([[bos_id] + token_ids_list])\n",
    "\n",
    "    # Disable gradient calculations for inference.\n",
    "    with torch.no_grad():\n",
    "        # Perform a forward pass. `labels` are provided for automatic loss calculation.\n",
    "        # The model internally shifts `labels` to align them with `logits` for calculating CrossEntropyLoss.\n",
    "        # For `input_ids = [BOS, t1, t2, ..., tN]`, the `labels` become `[BOS, t1, t2, ..., tN]`. \n",
    "        # The model computes loss for predicting t1 from BOS, t2 from [BOS, t1], ..., tN from [BOS, ..., tN-1].\n",
    "        # There are `len(token_ids_list)` such predictions.\n",
    "        outputs = model(model_input_ids, labels=model_input_ids)\n",
    "\n",
    "        # Calculate the total log likelihood of the original sentence (excluding BOS).\n",
    "        # outputs.loss is the *average* negative log likelihood per predicted token.\n",
    "        # The number of predicted tokens here is `len(token_ids_list)` (i.e., the original sentence length).\n",
    "        # So, total_log_likelihood = - (average_neg_log_likelihood) * num_predicted_tokens\n",
    "        log_likelihood = -outputs.loss.item() * len(token_ids_list)\n",
    "    return log_likelihood\n",
    "\n",
    "# Define a function to compute conditional log probabilities of each token in a sentence\n",
    "def compute_conditional_log_probabilities(sentence_text, model, tokenizer):\n",
    "    # For GPT-2, EOS token ID is used as BOS token ID.\n",
    "    bos_id = tokenizer.eos_token_id\n",
    "\n",
    "    # Tokenize the input sentence to get a list of token IDs.\n",
    "    token_ids_list = tokenizer(sentence_text, return_tensors=\"pt\")[\"input_ids\"][0].tolist()\n",
    "\n",
    "    # Create the model input by prepending the BOS token ID.\n",
    "    # model_input_ids will be [BOS, t_0, t_1, ..., t_{L-1}], where L is length of token_ids_list.\n",
    "    model_input_ids = torch.tensor([[bos_id] + token_ids_list])\n",
    "\n",
    "    # Perform a forward pass through the model to get logits.\n",
    "    # No labels are passed here as we want to manually compute conditional probabilities from logits.\n",
    "    with torch.no_grad():\n",
    "        outputs = model(model_input_ids)\n",
    "        # all_logits shape: [batch_size=1, sequence_length_with_BOS, vocab_size]\n",
    "        all_logits = outputs.logits \n",
    "\n",
    "    # Align logits with target tokens for conditional probability calculation.\n",
    "    # The logits at position `i` are used to predict the token at position `i+1`.\n",
    "    # So, `all_logits[0, j, :]` contains the logits for predicting `model_input_ids[0, j+1]`.\n",
    "    # We need logits for predicting tokens t_0, t_1, ..., t_{L-1} (the original sentence tokens).\n",
    "    # Logit for t_0 (model_input_ids[0,1]) is all_logits[0,0,:] (context: BOS).\n",
    "    # Logit for t_1 (model_input_ids[0,2]) is all_logits[0,1,:] (context: BOS, t_0).\n",
    "    # ...\n",
    "    # Logit for t_{L-1} (model_input_ids[0,L]) is all_logits[0,L-1,:] (context: BOS, t_0, ..., t_{L-2}).\n",
    "    # Thus, we take logits from index 0 up to the second to last token position.\n",
    "    shifted_prediction_logits = all_logits[0, :-1, :] # Shape: [sequence_length_orig_sentence, vocab_size]\n",
    "\n",
    "    # The target tokens are the original sentence tokens (excluding the BOS token from input).\n",
    "    # These are t_0, t_1, ..., t_{L-1}.\n",
    "    target_token_ids = model_input_ids[0, 1:] # Shape: [sequence_length_orig_sentence]\n",
    "\n",
    "    # Decode the target token IDs to strings for more readable output.\n",
    "    target_tokens_str_list = [tokenizer.decode(token_id) for token_id in target_token_ids]\n",
    "\n",
    "    conditional_log_probs_list = []\n",
    "    # Iterate over each token in the original sentence to calculate its conditional log probability.\n",
    "    for k in range(len(target_token_ids)):\n",
    "        # Get the logits corresponding to the prediction of the k-th target token.\n",
    "        current_logits_for_target = shifted_prediction_logits[k, :] # Shape: [vocab_size]\n",
    "        \n",
    "        # Apply log_softmax to the logits to get log probabilities over the vocabulary.\n",
    "        # dim=-1 ensures softmax is computed across the vocabulary dimension.\n",
    "        log_softmax_dist = torch.log_softmax(current_logits_for_target, dim=-1)\n",
    "        \n",
    "        # Get the ID of the actual k-th target token.\n",
    "        actual_target_token_id = target_token_ids[k]\n",
    "        \n",
    "        # Extract the log probability of the actual target token from the distribution.\n",
    "        token_log_prob = log_softmax_dist[actual_target_token_id].item()\n",
    "        \n",
    "        conditional_log_probs_list.append((target_tokens_str_list[k], token_log_prob))\n",
    "            \n",
    "    return conditional_log_probs_list\n",
    "\n",
    "# Define a list of example sentences to compare their probabilities.\n",
    "# These sentences are chosen to illustrate how the language model assigns probabilities:\n",
    "# - \"the mouse ate the cheese\" is grammatically correct and semantically plausible.\n",
    "# - \"the cheese ate the mouse\" is grammatically correct but semantically less plausible.\n",
    "# - \"mouse the the cheese ate\" is grammatically incorrect and semantically implausible.\n",
    "# We expect the model to assign higher probabilities to more plausible and grammatically correct sentences.\n",
    "sentences = [\n",
    "    \"the mouse ate the cheese\",\n",
    "    \"the cheese ate the mouse\",\n",
    "    \"mouse the the cheese ate\"\n",
    "]\n",
    "\n",
    "# Iterate through the sentences and print their log probabilities and probabilities.\n",
    "for sentence in sentences:\n",
    "    # Compute the log probability of the current sentence using the defined function.\n",
    "    # Pass the model and tokenizer explicitly now.\n",
    "    logp = compute_log_probability(sentence, model, tokenizer)\n",
    "    # Convert the log probability to actual probability using math.exp().\n",
    "    # Since log_probability = log(P), then P = exp(log_probability).\n",
    "    prob = math.exp(logp)\n",
    "    # Print the results.\n",
    "    print(f\"Sentence: '{sentence}'\")\n",
    "    print(f\"Log Probability: {logp:.4f}\")\n",
    "    print(f\"Probability: {prob:.2e}\")\n",
    "\n",
    "    # Compute and print conditional log probabilities for each token in the sentence.\n",
    "    conditional_probs = compute_conditional_log_probabilities(sentence, model, tokenizer)\n",
    "    print(f\"Conditional Log Probabilities for '{sentence}':\")\n",
    "    total_cond_log_prob = 0\n",
    "    for token_str, log_p_token in conditional_probs:\n",
    "        # Ensure token_str is properly escaped for printing, e.g., if it contains special characters.\n",
    "        # Python's f-string handles most cases well. For extreme cases, repr() might be useful.\n",
    "        print(f\"  Log P('{token_str}' | ...): {log_p_token:.4f}\")\n",
    "        total_cond_log_prob += log_p_token\n",
    "    print(f\"  Sum of conditional log_probs: {total_cond_log_prob:.4f}\") # For verification\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
