{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8916864c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "# transformers: Provides pre-trained models and tokenizers like GPT-2.\n",
    "#   GPT2LMHeadModel is the GPT-2 model with a language modeling head.\n",
    "#   GPT2Tokenizer is used to convert text into tokens that the model can understand.\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "# torch: PyTorch library, used for tensor computations and neural network modeling.\n",
    "import torch\n",
    "# math: Standard Python library for mathematical functions, used here for math.exp().\n",
    "import math\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "# model_name specifies which version of GPT-2 to use (e.g., \"gpt2\", \"gpt2-medium\").\n",
    "model_name = \"gpt2\"\n",
    "# Load the tokenizer associated with the specified GPT-2 model.\n",
    "# The tokenizer converts input strings into a format (token IDs) understandable by the model.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "# Load the pre-trained GPT-2 model (GPT2LMHeadModel for language modeling tasks).\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "# Set the model to evaluation mode.\n",
    "# This disables layers like dropout and batch normalization that behave differently during training and inference.\n",
    "# It's crucial for getting deterministic and correct results when using the model for inference.\n",
    "model.eval()\n",
    "\n",
    "# Define a function to compute the log probability of a sentence\n",
    "def compute_log_probability(sentence):\n",
    "    # Tokenize the input sentence.\n",
    "    # - The tokenizer converts the sentence string into a sequence of token IDs.\n",
    "    # - return_tensors=\"pt\" specifies that the output should be PyTorch tensors.\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "\n",
    "    # Disable gradient calculations.\n",
    "    # torch.no_grad() is used because we are doing inference (calculating probability),\n",
    "    # not training the model. This reduces memory consumption and speeds up computation\n",
    "    # as gradients are not needed.\n",
    "    with torch.no_grad():\n",
    "        # Perform a forward pass through the model.\n",
    "        # **inputs unpacks the dictionary returned by the tokenizer (input_ids, attention_mask, etc.)\n",
    "        # as arguments to the model.\n",
    "        # labels=inputs[\"input_ids\"]: By providing 'labels' (which are the same as input_ids for causal LM),\n",
    "        # the model automatically calculates the CrossEntropyLoss between its predictions and the true tokens.\n",
    "        # The model predicts the next token at each position and compares it to the actual token provided in 'labels'.\n",
    "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "\n",
    "        # Calculate the total log likelihood of the sentence.\n",
    "        # outputs.loss: This is the average negative log likelihood per token (CrossEntropyLoss).\n",
    "        #   The loss is calculated as -1/N * sum_{i=1 to N} log P(token_i | preceding_tokens),\n",
    "        #   where N is the sequence length.\n",
    "        # inputs[\"input_ids\"].size(1): This gives the length of the token sequence (N).\n",
    "        # To get the total log likelihood for the entire sentence, we multiply the negative average log likelihood\n",
    "        # by the sequence length and then negate it again.\n",
    "        # So, log_likelihood = - (average_negative_log_likelihood) * sequence_length\n",
    "        #                  = - (-1/N * sum log P(token_i | ...)) * N\n",
    "        #                  = sum log P(token_i | ...)\n",
    "        # This sum represents the log of the joint probability of the sequence:\n",
    "        # log P(w_1, w_2, ..., w_n) = sum_{i=1 to n} log P(w_i | w_1, ..., w_{i-1}).\n",
    "        # The model's loss is essentially the negative average of these conditional log probabilities.\n",
    "        log_likelihood = -outputs.loss.item() * inputs[\"input_ids\"].size(1)\n",
    "    return log_likelihood\n",
    "\n",
    "# Define a list of example sentences to compare their probabilities.\n",
    "# These sentences are chosen to illustrate how the language model assigns probabilities:\n",
    "# - \"the mouse ate the cheese\" is grammatically correct and semantically plausible.\n",
    "# - \"the cheese ate the mouse\" is grammatically correct but semantically less plausible.\n",
    "# - \"mouse the the cheese ate\" is grammatically incorrect and semantically implausible.\n",
    "# We expect the model to assign higher probabilities to more plausible and grammatically correct sentences.\n",
    "sentences = [\n",
    "    \"the mouse ate the cheese\",\n",
    "    \"the cheese ate the mouse\",\n",
    "    \"mouse the the cheese ate\"\n",
    "]\n",
    "\n",
    "# Iterate through the sentences and print their log probabilities and probabilities.\n",
    "for sentence in sentences:\n",
    "    # Compute the log probability of the current sentence using the defined function.\n",
    "    logp = compute_log_probability(sentence)\n",
    "    # Convert the log probability to actual probability using math.exp().\n",
    "    # Since log_probability = log(P), then P = exp(log_probability).\n",
    "    prob = math.exp(logp)\n",
    "    # Print the results.\n",
    "    print(f\"Sentence: '{sentence}'\")\n",
    "    print(f\"Log Probability: {logp:.4f}\")\n",
    "    print(f\"Probability: {prob:.2e}\")\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
